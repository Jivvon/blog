---
name: 정지원
avatar: /static/images/avatar.png
occupation: DevOps Engineer
company: Channel.io
email: jwjeong127@gmail.com
linkedin: https://www.linkedin.com/in/jwjeong
github: https://github.com/Jivvon
---


## 소개

**문제를 해결하는 데에 즐거움을 느끼는** 데브옵스 엔지니어입니다.

**상황에 맞는 최선의 선택**을 하기 위해 고민합니다.<br />
프로젝트에 할당된 기간, 확장성, 지속 가능성, 비용 효율성을 고려하여 아키텍처를 설계하고 기술을 활용합니다.

기술이 가진 **책임**과 **영향력**을 이해하고, 조심스럽지만 과감하게 행동합니다.<br />
이를 위해 단순히 기술을 사용하는 것을 넘어, 근본적인 원리를 이해하기 위해 노력합니다.

문제를 분석하고, 원인을 파악하여 해결하는 과정을 좋아합니다.<br />
특히, 다양한 관점으로 문제를 바라보고 여러 가능한 해결 방법에 대해 토론하는 것에서 즐거움을 느낍니다.<br />
&nbsp; → [Kubernetes 환경에서 WebRTC 서비스의 묵음 현상 분석기](#블로깅)

일을 찾아서 움직이는 성향 덕분에, 자연스럽게 일의 **우선순위를 수시로 조정**하며 진행합니다.<br />
주어진 업무 외에도 팀에 도움이 되는 방향이 있다면 먼저 제안하고 움직이며, 전체적인 효율과 팀의 방향성을 함께 고려하여 기술적인 의사결정을 내립니다.


## 커리어

### Channel Corporation - DevOps Engineer [2022.05 ~ ]

입사 직후 Kubernetes 도입 및 서비스 운영 환경 설계를 주도하였고, 기존 ECS 기반의 인메모리 DB 및 메인 서버를 포함한 전체 시스템을 EKS로 성공적으로 마이그레이션하였습니다.<br />
Meet팀 소속으로 채널톡 전화 서비스 관련 DevOps 업무를 수행하며 WebRTC 서버 구축, 아키텍처 설계 참여, STT 비용 최적화 및 속도 개선을 이루었습니다.<br />
Terraform CI에서 권한 확인을 위한 어플리케이션 개발, 오퍼레이터 패턴으로 개발된 내부 배포 시스템의 기능 추가 등 개발 업무에도 참여하였습니다.<br />
현재는 글로벌 인프라 확장 및 멀티 클러스터 환경을 고려한 프로비저닝 자동화와 아키텍처 설계를 진행하고 있습니다.

### SI Analytics - DevOps Engineer [2021.03 ~ 2022.02]

온프레미스 환경에서 총 21대의 물리 서버(CPU + GPU)와 44대의 VM으로 구성된 5개의 Kubernetes 클러스터를 구축 및 운영하였습니다.<br />
각 클러스터는 Control Plane 노드 3대를 구성하고 HAProxy를 활용해 고가용성을 확보하였으며,
etcd 및 쿠버네티스 리소스에 대한 주기적인 백업과 복원이 가능한 환경을 구성하였습니다.<br />
또한, 클러스터 전반에 적용될 정책을 설계하고, Admission Controller를 활용하여 보안성과 일관성을 강화하였습니다.<br />
Grafana, Loki, Prometheus, Alertmanager를 연동한 모니터링 시스템을 구축하였으며,
DCGM Exporter에 필요한 메트릭을 추가하고 직접 바이너리를 빌드하여 GPU 관련 정보를 연구자들에게 제공하였습니다.


## 프로젝트

### [채널톡] - ECS에서 EKS로의 마이그레이션 프로젝트 리드

#### 인메모리 데이터베이스 마이그레이션

- 배경
    - ECS에서 모든 서비스를 EKS로 이전하는 전사적 마이그레이션을 수행함
    - 실시간으로 동작하는 [인메모리 데이터베이스](https://channel.io/ko/blog/articles/channel-v4-search-4d895b08)와 이를 처리하는 Gateway 역시 이전 대상에 포함되어 있었으며, 서비스의 실시간 응답성과 신뢰성을 유지하는 것이 핵심 요구사항이었음
    - 인메모리 데이터 갱신은 DynamoDB Streams를 기반으로 하며, 컨테이너 재기동 시 DynamoDB로부터 초기 데이터를 로딩하도록 되어 있어 DynamoDB Read 부하 또한 고려가 필요했음

- 주요 조치
    - ECS 및 EKS 서비스를 동일 도메인에서 호출 가능하도록 구성하고, 서비스 디스커버리를 Cloud Map에서 ALB 기반으로 통일함
    - 내부 직원용 채널을 대상으로 Header 및 URL Path 조건을 활용한 Canary Testing 환경을 구축하여 실제 서비스 트래픽 적용 전 QA를 완료함
    - ALB Listener Rule의 Weight 조절을 통해 약 1주일간 점진적 트래픽 이전을 수행, 안정적으로 전체 서비스를 EKS로 전환함
    - 컨테이너당 하나의 DB 타입만 처리하는 구조적 특성을 고려하여, 동일한 DB 타입 컨테이너는 서로 다른 노드에 스케줄링되도록 설정함

- 기술 부채 및 개선 인식
    - 현재 구조에서는 DynamoDB 이벤트 유실을 방지하기 위해 DynamoDB Streams를 ECS와 EKS로 둘 다 보내주었으나, 이벤트를 보존하거나 재처리하는 구조는 미흡함
    - Redis, SQS, Kafka 등의 중간 큐를 도입한 Pub/Sub 아키텍처였다면 일시적 장애나 재배포 시에도 이벤트 보존 및 유연한 재처리가 가능했을 것으로 판단됨

#### AWS Client 초기화 오류 해결 및 표준화 기여

- 배경
    - ECS에서 EKS로 마이그레이션하며 IAM 권한을 가져오지 못하는 문제가 발생했음
    - 동일한 코드의 ECS 환경에서는 해당 문제 발생하지 않음
    - 테스트 용도로 AWS Client 초기화 시, Custom Endpoint Resolver를 사용하고 있었음
- 원인 분석 및 해결
    - 로그 분석을 통해 Node Role을 Assume하는 것을 인지하고, AWS SDK 코드 레벨에서 근본 원인 파악했음
    - Custom Endpoint Resolver가 STS 요청에 대한 Endpoint를 덮어썼음을 확인함
    - ECS 환경에서는 기본 Credentials Provider Chain에서 STS보다 우선하는 ECS Task Metadata Endpoint를 통해 자격 증명을 가져오므로 해당 문제 발생하지 않았음
    - 동일 패턴을 사용하는 타 프로젝트에 해결 방안을 공유하고 수정 가이드를 제공하여, 전체 개발팀의 불필요한 트러블슈팅 시간을 절감하고 개발 효율 증대에 기여했음

#### 기타 작업

- 모든 애플리케이션에 적용 가능한 공용 Helm Chart 개발
    - 지속적으로 고도화하여 현재 약 80여개의 애플리케이션에서 활용 중
- Monorepo 기반 Helm Chart Repository 및 CI/CD 자동화 구축
    - GitHub Actions 기반의 CI 파이프라인을 구축하여 Monorepo 내 Helm Chart의 빌드, 테스트, AWS ECR 업로드를 자동화하고, SemVer를 통해 버전을 관리함




### [채널톡] - Meet 서비스

#### 내부망 고객사를 위한 WebRTC 서버 EIP 동적 할당 시스템 구축

- 배경
    - 내부망을 사용하는 고객사는 방화벽으로 인해 전화 서비스 이용이 제한되었음
    - 스케일링되는 WebRTC 서버의 가변적인 Public IP로 인해 특정 IP만을 허용할 수 없는 상황
- 해결
    - 고정 IP Pool 관리 방식과 TURN 서버 도입 방식 중, 고정 IP Pool 관리 방식을 채택
    - 당시 전화 서비스는 채널톡 신규 기능으로 안정화가 좀 더 필요한 단계였는데, 새로운 서버를 도입하는 것과 홉이 늘어나는 것에 대해 회의적이었음
    - 이러한 이유로 특정 대역의 AWS Elastic IP Pool을 확보하고 WebRTC 서버의 InitContainer에서 EIP를 동적으로 할당하도록 구성함
    - EIP 할당 과정에서 발생할 수 있는 동시성 문제를 해결하기 위해 Redis를 활용한 분산 스핀락을 구현하여 EIP 할당부터 유효성 검증까지 전 과정의 안정성을 확보
    - 운영 측면에서는 잠재적인 트래픽 급증에 대비하기 위해 스케일링 기준을 완화하고, 이슈 발생 시 즉각적인 인지를 위한 알람을 설정하여 서비스 안정성을 확보함

#### LiveKit Egress 스케일링 기준 수립

- 배경
    - 음성 기능만 사용하는 상황에서 `README`의 기본 리소스 설정은 **CPU 리소스의 약 15~30%만 실제 사용**해 상당한 CPU 낭비 발생함
    - LiveKit Egress는 내부적으로 CPU Capacity와 Usage를 직접 관리하였기 때문에 Kubernetes HPA와 **메커니즘이 충돌**하였음
    - Egress 서버가 각 워커(worker)의 어피니티(Affinity)를 질의하고 가장 높은 어피니티를 반환하는 워커 노드로 요청을 계속 라우팅하여, 요청 처리 중인 인스턴스로 리소스 용량이 가득 찰 때까지 요청 집중되는 현상 발생함.

- 최적화 및 결과
    - 서비스 기획 단계에서 정의된 **최대 사용자 및 룸(Room) 수를 기준으로 부하 테스트를 수행**함.
    - 스케일링 메커니즘 충돌을 해결하기 위해, 각 스케일링 기준 변수를 직접 통제함. K8s HPA의 경우 **CPU 사용률(Util)**을, Egress의 경우 **트랙(track) 및 룸(room)별 복합 CPU 코스트(composite CPU cost)**를 통제하며 실험 진행함.
    - 분석을 통해 **CPU 사용률이 요청(Request) 대비 평균 60%를 유지**하도록 최적의 스케일링 기준 수립함.
    - 스로틀링(Throttling) 발생이 민감한 서비스 특성 고려, **항상 1~2대의 여유 서버를 예비(reserve)로 확보**하는 정책 적용함.
    - 결과적으로 수립된 스케일링 기준은 현재까지 **큰 수정 없이 안정적으로 운영 중**이며, CPU 리소스 효율성을 극대화하여 비용 절감에 기여함.



- WebRTC 녹음 서버에서 내부적으로 리소스를 관리하여 스케일링 정책을 정하기 어려웠음
    녹음 서버는 cgroup의
    녹음 역할을 하는 worker 서버를 선택하는 통신 프로토콜에서 로직을 파악한 후, 부하테스트를 통해 스케일링 정책을 정함

#### WebRTC 서버 카나리 배포

- WebRTC 서버 업그레이드 카나리 배포 썰
    WebRTC 서버 업데이트를 심리스하게 하기 위한 카나리 배포 전략을 제안함
    같은 방의 피어는 모두 동일한 버전을 사용해야 하는 제약사항으로 인해 어플리케이션 수준에서 다른 버전의 서버로 적절히 라우팅
    비즈니스 영향도를 고려하여 주요 고객사는 따로 관리
    재배포 없이 빠른 롤백을 위해 외부 Redis를 이용하여 런타임에 rate를 조절할 수 있도록 구성

#### 아키텍처 설계

- MSA 구조로 전환하기 위한 설계
    외부(고객사의 컴퓨터, 고객사의 물리 전화, 엔드 유저의 통신사)와 통신하는 서버들을 고려한 토폴로지 설계
    확장성과 가용성을 고려하여 RTP 프로토콜 중계 서버 제안
    Peer와 직접 통신하는 서버의 경우 hostNetwork를 사용하여 노드의 특정 UDP 대역을 사용하도록 설정
    통신사와 고정된 Port로 통신하기 위해 SNAT를 비활성화하는 CNI Plugin 설정 적용
    Media 서버의 요청 타입 및 내부 알고리즘을 고려하여 로드 밸런싱 & 오토 스케일링 되도록 최적화
    전화 연동 관련 인프라 CI/CD 파이프라인 구성, Helm Chart 작성 및 배포




### Terraform

#### terraform module to s3 and versioning & seperate states
    - 문제점
        - 환경별로 분리된 state가 너무 커서 전체 plan 시간이 오래걸렸고, 그러다보니 target을 지정해서 작업하게 됨. 작업자가 여럿이고 싱크를 놓치는 경우가 쌓이며 생산성이 저하됨
        - Region별로 state가 분리되어 있었음
        - 모든 환경에서 모듈을 공통으로 사용하기 때문에, 모듈 수정에 대한 부담이 굉장히 컸음
        - 팀원들은 환경별 권한이 다르기 때문에 production 환경에 대한 작업인 경우 상급자가 보틀넥이 되었음
        - CI/CD 파이프라인의 부재로 동시에 여러명이 작업하기 불편했음
        - 중복 코드가 너무 많았음
    - 해결
        - Terragrunt를 도입하여 state를 세부적으로 나누고, 여러 작업자가 다른 환경에 동시에 작업 가능하도록 개선
        - Terragrunt 도입과 Git Repository 구조 변경으로 Account, Region, Stage 등 특정 범위 내에서 코드를 재사용할 수 있도록 개선
        - 모듈을 버저닝하여 AWS S3에서 관리하도록 함
        - CI/CD 도구로써 Atlantis를 도입함. atlantis와 terragrunt가 포함된 이미지를 직접 빌드하여 사용함
        - atlantis에서 권한 확인을 위한 tool을 Go로 개발함. Github Webhook에서 확인 가능한 정보를 Okta에서 관리하는 기준에 맞춰 인가 여부를 판단함


#### Atlantis CI/CD w/Terragrunt
- atlantis-permission-checker


### 모니터링 시스템
  - recording rule
        복잡한 대시보드 쿼리 최적화: 대시보드에서 주기적으로 실행되는 복잡한 쿼리를 Recording Rule로 미리 계산하여 저장함으로써 대시보드 로딩 시간을 단축했습니다
        대시보드에서 수십 개의 패널이 동일한 복잡한 집계 쿼리를 반복 실행하면서 Prometheus 서버에 과부하가 발생했습니다. 이 문제를 해결하기 위해 자주 사용되는 계산식을 Recording Rule로 변환하여 미리 계산된 결과를 새로운 시계열로 저장했습니다. 예를 들어 `sum by (method, request_uri)(rate(nginx_http_response_time_seconds_hist_sum1m)) / sum by (method, request_uri)(rate(nginx_http_response_time_seconds_hist_count1m))` 같은 복잡한 쿼리를 `job:nginx_http_response_time_seconds:avg_1m`이라는 단일 메트릭으로 미리 계산해 저장했습니다.

        샘플 수 초과 문제 해결: 대규모 클러스터에서 `query.max-samples` 설정값 초과로 쿼리 실행이 차단되는 상황이 발생했습니다.
        고해상도 메트릭을 쿼리할 때 샘플 수 제한으로 인해 쿼리가 실패했습니다. Recording Rule을 통해 원본 데이터를 미리 집계하여 샘플 수를 크게 줄였고, 이를 통해 쿼리 성공률을 높이고 응답 시간을 개선했습니다.

  - cardinality
        Cardinality 관리를 통한 리소스 사용 최적화
        초기 설정에서 모든 Kubernetes 파드의 이름을 ‘instance’ 레이블로 추가하여 메트릭을 수집했는데, 이로 인해 높은 카디널리티가 발생했습니다. 네임스페이스를 대상으로 하도록 설정을 변경하고 불필요한 고카디널리티 레이블을 드롭함으로써 문제를 해결했습니다. 이 조치로 메모리 사용량이 60% 감소하고 쿼리 지연 시간이 크게 개선되었습니다
  - tsdb status




## 블로깅

### [Kubernetes 환경에서 WebRTC 서비스의 묵음 현상 분석기](https://channel.io/ko/blog/articles/kubernetes-webrtc-07b0d3b3)

Kubernetes 환경의 Meet 서비스 스케일링 과정에서 발생한 WebRTC 묵음 현상에 대해 원인을 분석하고 해결해나가는 과정을 채널톡 기술블로그에 작성하였습니다


## 학력

### 충남대학교 - 컴퓨터공학과 [2015.02 ~ 2022.02]

학점 3.76/4.5 (전공 3.86/4.5)
